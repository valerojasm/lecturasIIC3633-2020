# Opinions on Combining Predictions for Accurate Recommender Systems
Author: Valentina Rojas

It's widely known that collaborative filtering algorithms are some of the most used and precise recommender algorithms today, with state-of-the-art research coming out periodically. Innovating in this area is not an easy task, especially with such robust techniques to compete with. Jahrer et al propose a way to outperform popular CF methods by linearly combining their predictions on the same test data: an ensemble. The weights of said combination are given by a regularized linear regression.

My first contradicting point with this study is the computing cost of the ensembles. From all their experiments, the best one in terms of performance and accuracy is Bagging with NN, Polynomial Regression and GBDT - but this comes after several experiments mixing even more models. Each one of them is trained on a subset of the data, and then all of them are trained again on another subset of the remaining data in order to obtain the linear factors. In a highly scalable project this is just blatantly inefficient, since you would have a training time of at least the sum of the orders of each algorithm. They do tackle this problem somehow: they include bagging as the ensemble method; this design decision can also affect the quality of the recommendations, like any bootstrap sampling method the item coverage can decrease, so for anyone looking forward to implement a technique like this I would recommend to thoroughly analyze this trade-off, since it could be crucial with a very dense user/item database.

Since this paper was mainly supported on the Netflix Prize dataset, it didn't go deep in some of the typical problems recommender systems face, like cold start for example. [Chikhaoui et al](https://www.researchgate.net/publication/221191312_An_Improved_Hybrid_Recommender_System_by_Combining_Predictions) tried to address this dilemma by taking into account demographic information from the users in the ensemble, getting an increase of 0.155 in precision and a little over an 11% in coverage.

Going back to the high computing costs of this method - CF algorithms are costly per se. Yes, they are highly used, but again, for very large scales they can be very expensive to maintain. In my opinion, a hybrid combination between content based and CF could maybe be taken into account - I think it can give a very integral perspective and therefore increase the quality of recommendations like Chikhaoui et al suggest. This, could also be complemented with a different ensemble method. [Bar et al](http://www.bgu.ac.il/~shanigu/Publications/Improving%20Simple%20Collaborative%20Filtering%20Models%20Using%20Ensemble%20Methods.pdf) tested, apart from bagging and boosting, a fusion method, which seeks to apply a model on the same dataset several times but with different initial parameters. Since we would probably see an increase in performance with a hybrid approach, sampling could -maybe- no longer be needed, and in that sense, fusion would increase coverage since all data is seen in every epoch.