# Opinions on FunkSVD
Author: Valentina Rojas

This is a very well worded blog post explaining how they developed a machine learning algorithm to predict user ratings for movies for a Netflix contest, which anyone with a fairly decent background in anything involving higher level math can understand.

The main challenge here, and for me what was most fascinating, was to figure out how to abstract rich information from a simple 1-5 number mapping a specific user to a specific movie, how this simple piece of data can be of actual use to get an accurate prediction to what that user would like - and this is where it got interesting. Singular Value Decomposition is basically factorizing a matrix into singular vectors, in other words expanding it to the multiplication of other unique matrices. Even though I'm completely aware of the foundations of machine learning, it can't stop to amaze me. I just find it completely fascinating, how we can build such intricate models based on foundational mathematics to beautifully solve either the most basic of problems to using them for extremely high level applications.

I wish they could've gone more in depth on the construction of the SVD and the process of obtaining all the information to come up with it. For what I understood, Netflix only gave them a (User, Movie, Rating, Date) tuple-filled dataset, the pre-processing of choosing relevant tags for movies and abstracting the importance given by the user to these same features by studying the distribution of all their given ratings (alas, building the SVD itself) must have been fascinating. For what I read, SVD is a rather popular unsupervised learning algorithm, since it's a direct and simple way to try to decompose what something is made of - according to [this article](https://blog.statsbot.co/singular-value-decomposition-tutorial-52c695315254), this algorithm is basically the precursor of IQ: it was used to see what intelligence is made of in social sciences.

The way they talk of challenges we face as we build models like these ourselves as well is also quite funny to read. How sometimes we make decisions based on literally nothing, on how it just works: like them choosing a factor of 25 to get a reliable mean for the movie rating distribution out of thin air (it just works!).

I don't know if it's because this was in 2006, where I instantly think that these techniques where not as popular as they are now (I know they've been around forever, but they've gotten way more refined in the past 8 years at least) just made me think of how that whole thinking process must have been. Now we have so many libraries to help us take some burden off our shoulders and focus on the real issue, on the actual output of our project, on what our data is telling us, but when all of this didn't exist and we had to implement all the math by hand I think... Wow, that's actually not a burden at all. That's probably one of the best ways to actually connect with your data, to make sense of it all in a deeper way than we do today, where we have all the tools in the world available to us. Not that long ago I saw [an interview](https://www.deeplearning.ai/blog/hodl-andrej-karpathy/) to Andrej Karpathy, the ML director of Tesla, and he said something among the lines: if you really want to learn, then take the time to do the tedious stuff first, only then you'll really know what you're doing.